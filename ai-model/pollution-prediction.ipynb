{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62003fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 21\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mxgb_model\u001b[49m, f)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model saved as xgboost_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(\"✅ Model saved as xgboost_model.pkl\")\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Scaler saved as scaler.pkl\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" All libraries imported successfully!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv('pollution_data.csv')\n",
    "\n",
    "print(f\"\\n Dataset Shape: {df.shape}\")\n",
    "print(f\"\\n First few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\n Data Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\n Statistical Summary:\")\n",
    "print(df.describe())\n",
    "print(f\"\\n Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\n  Handling missing values...\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    print(\" Missing values handled\")\n",
    "else:\n",
    "    print(\"\\n No missing values found\")\n",
    "\n",
    "target_col = 'pollution_level'\n",
    "if target_col not in df.columns:\n",
    "    possible_targets = [col for col in df.columns if 'pollution' in col.lower() or 'pm' in col.lower() or 'aqi' in col.lower()]\n",
    "    if possible_targets:\n",
    "        target_col = possible_targets[0]\n",
    "        print(f\"\\n Using '{target_col}' as target variable\")\n",
    "\n",
    "exclude_cols = [target_col]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or 'id' in col.lower() or 'date' in col.lower() or 'time' in col.lower():\n",
    "        if col != target_col:\n",
    "            exclude_cols.append(col)\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\n Features ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\" Target: {target_col}\")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "print(f\"\\n Feature matrix shape: {X.shape}\")\n",
    "print(f\" Target shape: {y.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\n Training set: {X_train.shape[0]} samples\")\n",
    "print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"\\n Features scaled using StandardScaler\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: TRAINING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "print(\"\\n Training Linear Regression (Baseline)...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "models['Linear Regression'] = lr_model\n",
    "predictions['Linear Regression'] = lr_pred\n",
    "\n",
    "print(\" Linear Regression trained\")\n",
    "\n",
    "print(\"\\n Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)  \n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "models['Random Forest'] = rf_model\n",
    "predictions['Random Forest'] = rf_pred\n",
    "\n",
    "print(\" Random Forest trained\")\n",
    "\n",
    "print(\"\\n Training XGBoost (Primary Model)...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)  \n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "models['XGBoost'] = xgb_model\n",
    "predictions['XGBoost'] = xgb_pred\n",
    "\n",
    "print(\" XGBoost trained\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name, pred in predictions.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    \n",
    "    metrics[model_name] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n {model_name}:\")\n",
    "    print(f\"   RMSE: {rmse:.2f}\")\n",
    "    print(f\"   MAE:  {mae:.2f}\")\n",
    "    print(f\"   R²:   {r2:.4f}\")\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "best_model_name = metrics_df['R²'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\n Best Model: {best_model_name} (R² = {metrics_df.loc[best_model_name, 'R²']:.4f})\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: SHAP ANALYSIS (Feature Attribution)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n Calculating SHAP values for XGBoost model...\")\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "sample_size = min(500, len(X_test))\n",
    "X_test_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\" SHAP values calculated for {sample_size} samples\")\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n Top 10 Most Important Features:\")\n",
    "print(shap_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: FEATURE IMPORTANCE VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_n = min(15, len(shap_importance))\n",
    "plt.barh(range(top_n), shap_importance.head(top_n)['importance'].values)\n",
    "plt.yticks(range(top_n), shap_importance.head(top_n)['feature'].values)\n",
    "plt.xlabel('Mean |SHAP Value| (Average Impact on Model Output)', fontsize=12)\n",
    "plt.title('Feature Importance (SHAP Analysis)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature importance chart displayed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: GENERATING 48-HOUR PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "current_time = datetime.now()\n",
    "future_timestamps = [current_time + timedelta(hours=i) for i in range(48)]\n",
    "\n",
    "future_data = []\n",
    "\n",
    "for timestamp in future_timestamps:\n",
    "    hour = timestamp.hour\n",
    "    day_of_week = timestamp.weekday()\n",
    "    is_weekend = 1 if day_of_week >= 5 else 0\n",
    "    is_rush_hour = 1 if hour in [7, 8, 9, 17, 18, 19] else 0\n",
    "    \n",
    "    feature_dict = {}\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col == 'hour':\n",
    "            feature_dict[col] = hour\n",
    "        elif col == 'day_of_week':\n",
    "            feature_dict[col] = day_of_week\n",
    "        elif col == 'is_weekend':\n",
    "            feature_dict[col] = is_weekend\n",
    "        elif col == 'is_rush_hour':\n",
    "            feature_dict[col] = is_rush_hour\n",
    "        elif 'traffic' in col.lower() or 'congestion' in col.lower():\n",
    "          \n",
    "            base_value = X[col].mean()\n",
    "            feature_dict[col] = base_value * (1.5 if is_rush_hour else 0.8)\n",
    "        elif 'temperature' in col.lower():\n",
    "            \n",
    "            base_temp = X[col].mean()\n",
    "            feature_dict[col] = base_temp + 5 * np.sin(2 * np.pi * hour / 24)\n",
    "        else:\n",
    "            \n",
    "            feature_dict[col] = X[col].mean() + np.random.normal(0, X[col].std() * 0.1)\n",
    "    \n",
    "    future_data.append(feature_dict)\n",
    "\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "\n",
    "future_predictions = xgb_model.predict(future_df)\n",
    "\n",
    "test_residuals = y_test - xgb_pred\n",
    "residual_std = np.std(test_residuals)\n",
    "confidence_lower = future_predictions - 1.96 * residual_std\n",
    "confidence_upper = future_predictions + 1.96 * residual_std\n",
    "\n",
    "future_predictions = np.clip(future_predictions, 0, 300)\n",
    "confidence_lower = np.clip(confidence_lower, 0, 300)\n",
    "confidence_upper = np.clip(confidence_upper, 0, 300)\n",
    "\n",
    "print(f\"\\n Generated 48-hour predictions\")\n",
    "print(f\"   Mean prediction: {future_predictions.mean():.2f}\")\n",
    "print(f\"   Range: {future_predictions.min():.2f} - {future_predictions.max():.2f}\")\n",
    "\n",
    "\n",
    "def categorize_aqi(value):\n",
    "    \"\"\"Categorize pollution level into AQI categories\"\"\"\n",
    "    if value <= 50:\n",
    "        return 'Good'\n",
    "    elif value <= 100:\n",
    "        return 'Moderate'\n",
    "    elif value <= 150:\n",
    "        return 'Unhealthy for Sensitive Groups'\n",
    "    elif value <= 200:\n",
    "        return 'Unhealthy'\n",
    "    elif value <= 300:\n",
    "        return 'Very Unhealthy'\n",
    "    else:\n",
    "        return 'Hazardous'\n",
    "\n",
    "\n",
    "aqi_categories = [categorize_aqi(val) for val in future_predictions]\n",
    "\n",
    "print(\"\\n AQI Category Distribution (48-hour forecast):\")\n",
    "category_counts = pd.Series(aqi_categories).value_counts()\n",
    "print(category_counts.to_string())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 9: EXPORTING DATA FOR WEBAPP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "predictions_export = pd.DataFrame({\n",
    "    'timestamp': future_timestamps,\n",
    "    'hour': [t.hour for t in future_timestamps],\n",
    "    'day': [t.strftime('%A') for t in future_timestamps],\n",
    "    'pollution_prediction': future_predictions,\n",
    "    'confidence_lower': confidence_lower,\n",
    "    'confidence_upper': confidence_upper,\n",
    "    'aqi_category': aqi_categories\n",
    "})\n",
    "\n",
    "predictions_export.to_csv('predictions_48h.csv', index=False)\n",
    "print(\"\\n Exported: predictions_48h.csv\")\n",
    "print(f\"   Shape: {predictions_export.shape}\")\n",
    "print(predictions_export.head(3))\n",
    "\n",
    "\n",
    "feature_importance_export = shap_importance.copy()\n",
    "feature_importance_export['importance_percentage'] = (\n",
    "    feature_importance_export['importance'] / feature_importance_export['importance'].sum() * 100\n",
    ")\n",
    "\n",
    "feature_importance_export.to_csv('feature_importance.csv', index=False)\n",
    "print(\"\\n Exported: feature_importance.csv\")\n",
    "print(f\"   Shape: {feature_importance_export.shape}\")\n",
    "print(feature_importance_export.head(3))\n",
    "\n",
    "\n",
    "n_samples = min(15, len(X_test_sample))\n",
    "sample_indices = np.linspace(0, len(X_test_sample)-1, n_samples, dtype=int)\n",
    "\n",
    "shap_samples = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample_idx = X_test_sample.index[idx]\n",
    "    base_value = explainer.expected_value\n",
    "    prediction = xgb_model.predict(X_test_sample.iloc[[idx]])[0]\n",
    "    actual = y_test.loc[sample_idx]\n",
    "    \n",
    "    \n",
    "    sample_row = {\n",
    "        'sample_id': f'sample_{idx+1}',\n",
    "        'actual_pollution': actual,\n",
    "        'predicted_pollution': prediction,\n",
    "        'base_value': base_value,\n",
    "        'error': abs(prediction - actual)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for feat_idx, feature in enumerate(feature_cols):\n",
    "        shap_value = shap_values[idx, feat_idx]\n",
    "        feature_value = X_test_sample.iloc[idx][feature]\n",
    "        sample_row[f'{feature}_value'] = feature_value\n",
    "        sample_row[f'{feature}_shap'] = shap_value\n",
    "    \n",
    "    shap_samples.append(sample_row)\n",
    "\n",
    "shap_samples_df = pd.DataFrame(shap_samples)\n",
    "shap_samples_df.to_csv('shap_sample_predictions.csv', index=False)\n",
    "\n",
    "print(\"\\n Exported: shap_sample_predictions.csv\")\n",
    "print(f\"   Shape: {shap_samples_df.shape}\")\n",
    "print(f\"   Columns: {len(shap_samples_df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" PIPELINE COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n MODEL PERFORMANCE:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   Test R²: {metrics_df.loc[best_model_name, 'R²']:.4f}\")\n",
    "print(f\"   Test RMSE: {metrics_df.loc[best_model_name, 'RMSE']:.2f}\")\n",
    "print(f\"   Test MAE: {metrics_df.loc[best_model_name, 'MAE']:.2f}\")\n",
    "\n",
    "print(\"\\n EXPORTED FILES:\")\n",
    "print(\"    predictions_48h.csv (48 rows × 7 columns)\")\n",
    "print(\"    feature_importance.csv (ranked features)\")\n",
    "print(\"    shap_sample_predictions.csv (15 detailed samples)\")\n",
    "\n",
    "print(\"\\n TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for i, row in feature_importance_export.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\n PREDICTION VALIDATION:\")\n",
    "print(f\"   48-hour average: {future_predictions.mean():.1f}\")\n",
    "print(f\"   Prediction range: {future_predictions.min():.1f} - {future_predictions.max():.1f}\")\n",
    "print(f\"   Most common AQI: {pd.Series(aqi_categories).mode()[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" ALL SYSTEMS READY FOR HACKATHON DEMO!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "print(\"\\n Quick Sanity Checks:\")\n",
    "check_passed = True\n",
    "\n",
    "if predictions_export['pollution_prediction'].between(0, 300).all():\n",
    "    print(\"    All predictions within valid range [0-300]\")\n",
    "else:\n",
    "    print(\"     Some predictions outside valid range\")\n",
    "    check_passed = False\n",
    "\n",
    "if len(feature_importance_export) == len(feature_cols):\n",
    "    print(\"    All features have importance scores\")\n",
    "else:\n",
    "    print(\"     Missing feature importance data\")\n",
    "    check_passed = False\n",
    "\n",
    "if shap_samples_df['error'].mean() < metrics_df.loc[best_model_name, 'MAE'] * 1.5:\n",
    "    print(\"    SHAP samples representative of model performance\")\n",
    "else:\n",
    "    print(\"    SHAP samples may not be representative\")\n",
    "    check_passed = False\n",
    "\n",
    "if check_passed:\n",
    "    print(\"\\n All validation checks passed!\")\n",
    "else:\n",
    "    print(\"\\n  Some validation checks failed - review output above\")\n",
    "\n",
    "print(\"\\n Ready for integration with React frontend!\")\n",
    "print(\" Use the exported CSVs to build your visualization dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51587b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
